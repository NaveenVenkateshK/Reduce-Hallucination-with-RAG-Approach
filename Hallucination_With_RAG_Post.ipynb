{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HSTJqVlrqwzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da2bc85-0030-47cc-8ae7-683713e7685e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q ctransformers langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from langchain.tools import Tool\n",
        "from langchain.llms import CTransformers\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "\n",
        "class HallucinationRAG():\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "      \"\"\"\n",
        "      Initializes the HallucinationRAG class.\n",
        "\n",
        "      This constructor sets up the necessary components for the system, including the language model (LLM)\n",
        "      and user query.\n",
        "\n",
        "      Parameters:\n",
        "          None\n",
        "\n",
        "      Returns:\n",
        "          None\n",
        "      \"\"\"\n",
        "\n",
        "      # Initialize the language model (LLM) using the CTransformers class\n",
        "      self.llm = CTransformers(model=\"TheBloke/Llama-2-7B-Chat-GGML\", model_file = 'llama-2-7b-chat.ggmlv3.q2_K.bin', callbacks=[StreamingStdOutCallbackHandler()])\n",
        "\n",
        "      # Set the default user query for the system\n",
        "      self.user_query = \"impact of Ms dhoni’s moon landing\"\n",
        "\n",
        "      # Set Google Search API credentials using environment variables\n",
        "      os.environ[\"GOOGLE_CSE_ID\"] = \"YOUR_GOOGLE_SEARCH_ENGINE_ID\"\n",
        "      os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"\n",
        "\n",
        "      # Set up logging\n",
        "      logging.basicConfig(filename='hallucination_rag.log', level=logging.INFO)\n",
        "\n",
        "\n",
        "    def model_response_without_RAG(self) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Generates a model response without using the RAG approach.\n",
        "\n",
        "        This method uses a predefined template and the language model to generate a response.\n",
        "\n",
        "        Parameters:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            str: The generated response.\n",
        "        \"\"\"\n",
        "\n",
        "        # Define the template for the response\n",
        "        template = \"\"\"\n",
        "        [INST] <<SYS>>\n",
        "        You are a helpful, respectful and honest assistant. Your answers are always brief.\n",
        "        <</SYS>>\n",
        "        {text}[/INST]\n",
        "        \"\"\"\n",
        "\n",
        "        # Create a PromptTemplate instance with the template and input variable\n",
        "        prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "\n",
        "        # Create an LLMChain instance with the prompt and language model\n",
        "        llm_chain = LLMChain(prompt=prompt, llm=self.llm)\n",
        "\n",
        "        try:\n",
        "            # Run the LLMChain with the user's query to generate a response\n",
        "            result = llm_chain.run(self.user_query)\n",
        "            logging.info(\"Generated response without RAG approach.\")\n",
        "\n",
        "            # Return the generated response\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle the exception by logging an error message\n",
        "            logging.error(f\"Error occurred in model_response_without_RAG: {str(e)}\")\n",
        "            return \"An error occurred while generating the response.\"\n",
        "\n",
        "    def _set_search_tool(self) -> Tool:\n",
        "\n",
        "        \"\"\"\n",
        "        Creates and returns a Google Search Tool.\n",
        "\n",
        "        Returns:\n",
        "            Tool: The Google Search Tool.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Create an instance of the GoogleSearchAPIWrapper to handle Google Search API interactions.\n",
        "            search = GoogleSearchAPIWrapper()\n",
        "\n",
        "            # Create a Tool instance with a name, description, and the search.run function as its operation.\n",
        "            tool = Tool(\n",
        "                name=\"Google Search\",\n",
        "                description=\"Search Google for recent results.\",\n",
        "                func=search.run,\n",
        "            )\n",
        "\n",
        "            # Return the Tool instance representing the Google Search tool.\n",
        "            return tool\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error occurred while setting up the search tool: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def _run_tool(self) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Executes the search tool to retrieve content from Google.\n",
        "\n",
        "        Returns:\n",
        "            str: The content retrieved from Google.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set up the Google Search Tool.\n",
        "        tool = self._set_search_tool()\n",
        "\n",
        "        try:\n",
        "\n",
        "          # Attempt to run the tool with the user's query to retrieve content from Google.\n",
        "          content = tool.run(self.user_query)\n",
        "\n",
        "          return content\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "          # Handle the exception here, you can print an error message or raise a custom exception\n",
        "          logging.error(f\"Error occurred while running the search tool: {str(e)}\")\n",
        "          return \"\"\n",
        "\n",
        "    def model_response_RAG_approach(self) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Generates a model response using the RAG approach.\n",
        "\n",
        "        This method retrieves content using the _run_tool method, then processes the content using\n",
        "        a predefined template and the language model.\n",
        "\n",
        "        Parameters:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            str: The generated response using the RAG approach.\n",
        "        \"\"\"\n",
        "\n",
        "        # Retrieve content using the _run_tool method.\n",
        "        content = self._run_tool()\n",
        "\n",
        "        # Create the template and LLMChain\n",
        "        template = \"\"\"\n",
        "        [INST] <<SYS>>\n",
        "        You are a helpful, respectful and honest assistant. Analayze the content and answer the user question.\n",
        "        <</SYS>>\n",
        "        {content}\n",
        "        Question:\"impact of Ms dhoni’s moon landing\"[/INST]\n",
        "        \"\"\"\n",
        "\n",
        "        # Set up a PromptTemplate with the template and input variables.\n",
        "        prompt = PromptTemplate(template=template, input_variables=[\"content\"])\n",
        "\n",
        "        # Create an LLMChain with the prompt and the language model.\n",
        "        llm_chain = LLMChain(prompt=prompt, llm=self.llm)\n",
        "\n",
        "        # Use the LLMChain to process the content\n",
        "        try:\n",
        "\n",
        "          # Use the LLMChain to process the content and generate a final response.\n",
        "          final_responce = llm_chain.run(content)\n",
        "          print(\"With RAG Approach:\\n\",final_responce)\n",
        "\n",
        "          return final_responce\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "          # Handle the exception here, you can print an error message or raise a custom exception\n",
        "          logging.error(f\"Error occurred while generating response with RAG approach: {str(e)}\")\n",
        "          return \"\""
      ],
      "metadata": {
        "id": "ixcszGAPrrVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd06c98c-fb67-4365-af34-e348a8c2b81f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hallucination_rag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    hallucination_rag = HallucinationRAG()\n",
        "    hallucination_rag.model_response_without_RAG()\n",
        "    hallucination_rag.model_response_RAG_approach()"
      ],
      "metadata": {
        "id": "5Up06ctlr21v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d358526e-fa60-4070-d7e8-7b0199bfc9a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    }
  ]
}